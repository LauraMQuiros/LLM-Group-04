{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-30T17:26:48.664327Z",
     "start_time": "2024-10-30T17:26:45.280580Z"
    }
   },
   "source": [
    "import pickle\n",
    "import torch\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import ngrams\n",
    "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
    "from peft import PeftModel\n",
    "from bert_score import score as BERT_score"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:27:15.392429Z",
     "start_time": "2024-10-30T17:27:08.304960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model, tokeniser and data from pkl\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(project_root)\n",
    "print(project_root)\n",
    "\n",
    "tokenizer_path = os.path.join(project_root, 'src/models/new_tokenizer/new_tokenizer')\n",
    "model_path = os.path.join(project_root, 'src/models/model')\n",
    "lora_model_path = os.path.join(project_root, 'src/models/lora_model')\n",
    "# data path is path + 'src/data/processed/data.pkl'\n",
    "data_path = os.path.join(project_root, 'data/processed/test.pkl')\n",
    "\n",
    "lora_tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "print(len(lora_tokenizer))\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map={\"\": \"cpu\"})\n",
    "base_model.resize_token_embeddings(len(lora_tokenizer))\n",
    "\n",
    "# Load the LoRA model on top of the base model\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_path, device_map={\"\": \"cpu\"})\n",
    "lora_model.eval()\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ],
   "id": "93521fe17a1e15aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04\n",
      "50261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/src/models/model were not used when initializing GPT2LMHeadModel: ['transformer.h.0.attn.c_attn.base_layer.bias', 'transformer.h.0.attn.c_attn.base_layer.weight', 'transformer.h.0.attn.c_attn.lora_A.default.weight', 'transformer.h.0.attn.c_attn.lora_B.default.weight', 'transformer.h.0.attn.c_proj.base_layer.bias', 'transformer.h.0.attn.c_proj.base_layer.weight', 'transformer.h.0.attn.c_proj.lora_A.default.weight', 'transformer.h.0.attn.c_proj.lora_B.default.weight', 'transformer.h.0.mlp.c_fc.base_layer.bias', 'transformer.h.0.mlp.c_fc.base_layer.weight', 'transformer.h.0.mlp.c_fc.lora_A.default.weight', 'transformer.h.0.mlp.c_fc.lora_B.default.weight', 'transformer.h.0.mlp.c_proj.base_layer.bias', 'transformer.h.0.mlp.c_proj.base_layer.weight', 'transformer.h.0.mlp.c_proj.lora_A.default.weight', 'transformer.h.0.mlp.c_proj.lora_B.default.weight', 'transformer.h.1.attn.c_attn.base_layer.bias', 'transformer.h.1.attn.c_attn.base_layer.weight', 'transformer.h.1.attn.c_attn.lora_A.default.weight', 'transformer.h.1.attn.c_attn.lora_B.default.weight', 'transformer.h.1.attn.c_proj.base_layer.bias', 'transformer.h.1.attn.c_proj.base_layer.weight', 'transformer.h.1.attn.c_proj.lora_A.default.weight', 'transformer.h.1.attn.c_proj.lora_B.default.weight', 'transformer.h.1.mlp.c_fc.base_layer.bias', 'transformer.h.1.mlp.c_fc.base_layer.weight', 'transformer.h.1.mlp.c_fc.lora_A.default.weight', 'transformer.h.1.mlp.c_fc.lora_B.default.weight', 'transformer.h.1.mlp.c_proj.base_layer.bias', 'transformer.h.1.mlp.c_proj.base_layer.weight', 'transformer.h.1.mlp.c_proj.lora_A.default.weight', 'transformer.h.1.mlp.c_proj.lora_B.default.weight', 'transformer.h.10.attn.c_attn.base_layer.bias', 'transformer.h.10.attn.c_attn.base_layer.weight', 'transformer.h.10.attn.c_attn.lora_A.default.weight', 'transformer.h.10.attn.c_attn.lora_B.default.weight', 'transformer.h.10.attn.c_proj.base_layer.bias', 'transformer.h.10.attn.c_proj.base_layer.weight', 'transformer.h.10.attn.c_proj.lora_A.default.weight', 'transformer.h.10.attn.c_proj.lora_B.default.weight', 'transformer.h.10.mlp.c_fc.base_layer.bias', 'transformer.h.10.mlp.c_fc.base_layer.weight', 'transformer.h.10.mlp.c_fc.lora_A.default.weight', 'transformer.h.10.mlp.c_fc.lora_B.default.weight', 'transformer.h.10.mlp.c_proj.base_layer.bias', 'transformer.h.10.mlp.c_proj.base_layer.weight', 'transformer.h.10.mlp.c_proj.lora_A.default.weight', 'transformer.h.10.mlp.c_proj.lora_B.default.weight', 'transformer.h.11.attn.c_attn.base_layer.bias', 'transformer.h.11.attn.c_attn.base_layer.weight', 'transformer.h.11.attn.c_attn.lora_A.default.weight', 'transformer.h.11.attn.c_attn.lora_B.default.weight', 'transformer.h.11.attn.c_proj.base_layer.bias', 'transformer.h.11.attn.c_proj.base_layer.weight', 'transformer.h.11.attn.c_proj.lora_A.default.weight', 'transformer.h.11.attn.c_proj.lora_B.default.weight', 'transformer.h.11.mlp.c_fc.base_layer.bias', 'transformer.h.11.mlp.c_fc.base_layer.weight', 'transformer.h.11.mlp.c_fc.lora_A.default.weight', 'transformer.h.11.mlp.c_fc.lora_B.default.weight', 'transformer.h.11.mlp.c_proj.base_layer.bias', 'transformer.h.11.mlp.c_proj.base_layer.weight', 'transformer.h.11.mlp.c_proj.lora_A.default.weight', 'transformer.h.11.mlp.c_proj.lora_B.default.weight', 'transformer.h.12.attn.c_attn.base_layer.bias', 'transformer.h.12.attn.c_attn.base_layer.weight', 'transformer.h.12.attn.c_attn.lora_A.default.weight', 'transformer.h.12.attn.c_attn.lora_B.default.weight', 'transformer.h.12.attn.c_proj.base_layer.bias', 'transformer.h.12.attn.c_proj.base_layer.weight', 'transformer.h.12.attn.c_proj.lora_A.default.weight', 'transformer.h.12.attn.c_proj.lora_B.default.weight', 'transformer.h.12.mlp.c_fc.base_layer.bias', 'transformer.h.12.mlp.c_fc.base_layer.weight', 'transformer.h.12.mlp.c_fc.lora_A.default.weight', 'transformer.h.12.mlp.c_fc.lora_B.default.weight', 'transformer.h.12.mlp.c_proj.base_layer.bias', 'transformer.h.12.mlp.c_proj.base_layer.weight', 'transformer.h.12.mlp.c_proj.lora_A.default.weight', 'transformer.h.12.mlp.c_proj.lora_B.default.weight', 'transformer.h.13.attn.c_attn.base_layer.bias', 'transformer.h.13.attn.c_attn.base_layer.weight', 'transformer.h.13.attn.c_attn.lora_A.default.weight', 'transformer.h.13.attn.c_attn.lora_B.default.weight', 'transformer.h.13.attn.c_proj.base_layer.bias', 'transformer.h.13.attn.c_proj.base_layer.weight', 'transformer.h.13.attn.c_proj.lora_A.default.weight', 'transformer.h.13.attn.c_proj.lora_B.default.weight', 'transformer.h.13.mlp.c_fc.base_layer.bias', 'transformer.h.13.mlp.c_fc.base_layer.weight', 'transformer.h.13.mlp.c_fc.lora_A.default.weight', 'transformer.h.13.mlp.c_fc.lora_B.default.weight', 'transformer.h.13.mlp.c_proj.base_layer.bias', 'transformer.h.13.mlp.c_proj.base_layer.weight', 'transformer.h.13.mlp.c_proj.lora_A.default.weight', 'transformer.h.13.mlp.c_proj.lora_B.default.weight', 'transformer.h.14.attn.c_attn.base_layer.bias', 'transformer.h.14.attn.c_attn.base_layer.weight', 'transformer.h.14.attn.c_attn.lora_A.default.weight', 'transformer.h.14.attn.c_attn.lora_B.default.weight', 'transformer.h.14.attn.c_proj.base_layer.bias', 'transformer.h.14.attn.c_proj.base_layer.weight', 'transformer.h.14.attn.c_proj.lora_A.default.weight', 'transformer.h.14.attn.c_proj.lora_B.default.weight', 'transformer.h.14.mlp.c_fc.base_layer.bias', 'transformer.h.14.mlp.c_fc.base_layer.weight', 'transformer.h.14.mlp.c_fc.lora_A.default.weight', 'transformer.h.14.mlp.c_fc.lora_B.default.weight', 'transformer.h.14.mlp.c_proj.base_layer.bias', 'transformer.h.14.mlp.c_proj.base_layer.weight', 'transformer.h.14.mlp.c_proj.lora_A.default.weight', 'transformer.h.14.mlp.c_proj.lora_B.default.weight', 'transformer.h.15.attn.c_attn.base_layer.bias', 'transformer.h.15.attn.c_attn.base_layer.weight', 'transformer.h.15.attn.c_attn.lora_A.default.weight', 'transformer.h.15.attn.c_attn.lora_B.default.weight', 'transformer.h.15.attn.c_proj.base_layer.bias', 'transformer.h.15.attn.c_proj.base_layer.weight', 'transformer.h.15.attn.c_proj.lora_A.default.weight', 'transformer.h.15.attn.c_proj.lora_B.default.weight', 'transformer.h.15.mlp.c_fc.base_layer.bias', 'transformer.h.15.mlp.c_fc.base_layer.weight', 'transformer.h.15.mlp.c_fc.lora_A.default.weight', 'transformer.h.15.mlp.c_fc.lora_B.default.weight', 'transformer.h.15.mlp.c_proj.base_layer.bias', 'transformer.h.15.mlp.c_proj.base_layer.weight', 'transformer.h.15.mlp.c_proj.lora_A.default.weight', 'transformer.h.15.mlp.c_proj.lora_B.default.weight', 'transformer.h.16.attn.c_attn.base_layer.bias', 'transformer.h.16.attn.c_attn.base_layer.weight', 'transformer.h.16.attn.c_attn.lora_A.default.weight', 'transformer.h.16.attn.c_attn.lora_B.default.weight', 'transformer.h.16.attn.c_proj.base_layer.bias', 'transformer.h.16.attn.c_proj.base_layer.weight', 'transformer.h.16.attn.c_proj.lora_A.default.weight', 'transformer.h.16.attn.c_proj.lora_B.default.weight', 'transformer.h.16.mlp.c_fc.base_layer.bias', 'transformer.h.16.mlp.c_fc.base_layer.weight', 'transformer.h.16.mlp.c_fc.lora_A.default.weight', 'transformer.h.16.mlp.c_fc.lora_B.default.weight', 'transformer.h.16.mlp.c_proj.base_layer.bias', 'transformer.h.16.mlp.c_proj.base_layer.weight', 'transformer.h.16.mlp.c_proj.lora_A.default.weight', 'transformer.h.16.mlp.c_proj.lora_B.default.weight', 'transformer.h.17.attn.c_attn.base_layer.bias', 'transformer.h.17.attn.c_attn.base_layer.weight', 'transformer.h.17.attn.c_attn.lora_A.default.weight', 'transformer.h.17.attn.c_attn.lora_B.default.weight', 'transformer.h.17.attn.c_proj.base_layer.bias', 'transformer.h.17.attn.c_proj.base_layer.weight', 'transformer.h.17.attn.c_proj.lora_A.default.weight', 'transformer.h.17.attn.c_proj.lora_B.default.weight', 'transformer.h.17.mlp.c_fc.base_layer.bias', 'transformer.h.17.mlp.c_fc.base_layer.weight', 'transformer.h.17.mlp.c_fc.lora_A.default.weight', 'transformer.h.17.mlp.c_fc.lora_B.default.weight', 'transformer.h.17.mlp.c_proj.base_layer.bias', 'transformer.h.17.mlp.c_proj.base_layer.weight', 'transformer.h.17.mlp.c_proj.lora_A.default.weight', 'transformer.h.17.mlp.c_proj.lora_B.default.weight', 'transformer.h.18.attn.c_attn.base_layer.bias', 'transformer.h.18.attn.c_attn.base_layer.weight', 'transformer.h.18.attn.c_attn.lora_A.default.weight', 'transformer.h.18.attn.c_attn.lora_B.default.weight', 'transformer.h.18.attn.c_proj.base_layer.bias', 'transformer.h.18.attn.c_proj.base_layer.weight', 'transformer.h.18.attn.c_proj.lora_A.default.weight', 'transformer.h.18.attn.c_proj.lora_B.default.weight', 'transformer.h.18.mlp.c_fc.base_layer.bias', 'transformer.h.18.mlp.c_fc.base_layer.weight', 'transformer.h.18.mlp.c_fc.lora_A.default.weight', 'transformer.h.18.mlp.c_fc.lora_B.default.weight', 'transformer.h.18.mlp.c_proj.base_layer.bias', 'transformer.h.18.mlp.c_proj.base_layer.weight', 'transformer.h.18.mlp.c_proj.lora_A.default.weight', 'transformer.h.18.mlp.c_proj.lora_B.default.weight', 'transformer.h.19.attn.c_attn.base_layer.bias', 'transformer.h.19.attn.c_attn.base_layer.weight', 'transformer.h.19.attn.c_attn.lora_A.default.weight', 'transformer.h.19.attn.c_attn.lora_B.default.weight', 'transformer.h.19.attn.c_proj.base_layer.bias', 'transformer.h.19.attn.c_proj.base_layer.weight', 'transformer.h.19.attn.c_proj.lora_A.default.weight', 'transformer.h.19.attn.c_proj.lora_B.default.weight', 'transformer.h.19.mlp.c_fc.base_layer.bias', 'transformer.h.19.mlp.c_fc.base_layer.weight', 'transformer.h.19.mlp.c_fc.lora_A.default.weight', 'transformer.h.19.mlp.c_fc.lora_B.default.weight', 'transformer.h.19.mlp.c_proj.base_layer.bias', 'transformer.h.19.mlp.c_proj.base_layer.weight', 'transformer.h.19.mlp.c_proj.lora_A.default.weight', 'transformer.h.19.mlp.c_proj.lora_B.default.weight', 'transformer.h.2.attn.c_attn.base_layer.bias', 'transformer.h.2.attn.c_attn.base_layer.weight', 'transformer.h.2.attn.c_attn.lora_A.default.weight', 'transformer.h.2.attn.c_attn.lora_B.default.weight', 'transformer.h.2.attn.c_proj.base_layer.bias', 'transformer.h.2.attn.c_proj.base_layer.weight', 'transformer.h.2.attn.c_proj.lora_A.default.weight', 'transformer.h.2.attn.c_proj.lora_B.default.weight', 'transformer.h.2.mlp.c_fc.base_layer.bias', 'transformer.h.2.mlp.c_fc.base_layer.weight', 'transformer.h.2.mlp.c_fc.lora_A.default.weight', 'transformer.h.2.mlp.c_fc.lora_B.default.weight', 'transformer.h.2.mlp.c_proj.base_layer.bias', 'transformer.h.2.mlp.c_proj.base_layer.weight', 'transformer.h.2.mlp.c_proj.lora_A.default.weight', 'transformer.h.2.mlp.c_proj.lora_B.default.weight', 'transformer.h.20.attn.c_attn.base_layer.bias', 'transformer.h.20.attn.c_attn.base_layer.weight', 'transformer.h.20.attn.c_attn.lora_A.default.weight', 'transformer.h.20.attn.c_attn.lora_B.default.weight', 'transformer.h.20.attn.c_proj.base_layer.bias', 'transformer.h.20.attn.c_proj.base_layer.weight', 'transformer.h.20.attn.c_proj.lora_A.default.weight', 'transformer.h.20.attn.c_proj.lora_B.default.weight', 'transformer.h.20.mlp.c_fc.base_layer.bias', 'transformer.h.20.mlp.c_fc.base_layer.weight', 'transformer.h.20.mlp.c_fc.lora_A.default.weight', 'transformer.h.20.mlp.c_fc.lora_B.default.weight', 'transformer.h.20.mlp.c_proj.base_layer.bias', 'transformer.h.20.mlp.c_proj.base_layer.weight', 'transformer.h.20.mlp.c_proj.lora_A.default.weight', 'transformer.h.20.mlp.c_proj.lora_B.default.weight', 'transformer.h.21.attn.c_attn.base_layer.bias', 'transformer.h.21.attn.c_attn.base_layer.weight', 'transformer.h.21.attn.c_attn.lora_A.default.weight', 'transformer.h.21.attn.c_attn.lora_B.default.weight', 'transformer.h.21.attn.c_proj.base_layer.bias', 'transformer.h.21.attn.c_proj.base_layer.weight', 'transformer.h.21.attn.c_proj.lora_A.default.weight', 'transformer.h.21.attn.c_proj.lora_B.default.weight', 'transformer.h.21.mlp.c_fc.base_layer.bias', 'transformer.h.21.mlp.c_fc.base_layer.weight', 'transformer.h.21.mlp.c_fc.lora_A.default.weight', 'transformer.h.21.mlp.c_fc.lora_B.default.weight', 'transformer.h.21.mlp.c_proj.base_layer.bias', 'transformer.h.21.mlp.c_proj.base_layer.weight', 'transformer.h.21.mlp.c_proj.lora_A.default.weight', 'transformer.h.21.mlp.c_proj.lora_B.default.weight', 'transformer.h.22.attn.c_attn.base_layer.bias', 'transformer.h.22.attn.c_attn.base_layer.weight', 'transformer.h.22.attn.c_attn.lora_A.default.weight', 'transformer.h.22.attn.c_attn.lora_B.default.weight', 'transformer.h.22.attn.c_proj.base_layer.bias', 'transformer.h.22.attn.c_proj.base_layer.weight', 'transformer.h.22.attn.c_proj.lora_A.default.weight', 'transformer.h.22.attn.c_proj.lora_B.default.weight', 'transformer.h.22.mlp.c_fc.base_layer.bias', 'transformer.h.22.mlp.c_fc.base_layer.weight', 'transformer.h.22.mlp.c_fc.lora_A.default.weight', 'transformer.h.22.mlp.c_fc.lora_B.default.weight', 'transformer.h.22.mlp.c_proj.base_layer.bias', 'transformer.h.22.mlp.c_proj.base_layer.weight', 'transformer.h.22.mlp.c_proj.lora_A.default.weight', 'transformer.h.22.mlp.c_proj.lora_B.default.weight', 'transformer.h.23.attn.c_attn.base_layer.bias', 'transformer.h.23.attn.c_attn.base_layer.weight', 'transformer.h.23.attn.c_attn.lora_A.default.weight', 'transformer.h.23.attn.c_attn.lora_B.default.weight', 'transformer.h.23.attn.c_proj.base_layer.bias', 'transformer.h.23.attn.c_proj.base_layer.weight', 'transformer.h.23.attn.c_proj.lora_A.default.weight', 'transformer.h.23.attn.c_proj.lora_B.default.weight', 'transformer.h.23.mlp.c_fc.base_layer.bias', 'transformer.h.23.mlp.c_fc.base_layer.weight', 'transformer.h.23.mlp.c_fc.lora_A.default.weight', 'transformer.h.23.mlp.c_fc.lora_B.default.weight', 'transformer.h.23.mlp.c_proj.base_layer.bias', 'transformer.h.23.mlp.c_proj.base_layer.weight', 'transformer.h.23.mlp.c_proj.lora_A.default.weight', 'transformer.h.23.mlp.c_proj.lora_B.default.weight', 'transformer.h.3.attn.c_attn.base_layer.bias', 'transformer.h.3.attn.c_attn.base_layer.weight', 'transformer.h.3.attn.c_attn.lora_A.default.weight', 'transformer.h.3.attn.c_attn.lora_B.default.weight', 'transformer.h.3.attn.c_proj.base_layer.bias', 'transformer.h.3.attn.c_proj.base_layer.weight', 'transformer.h.3.attn.c_proj.lora_A.default.weight', 'transformer.h.3.attn.c_proj.lora_B.default.weight', 'transformer.h.3.mlp.c_fc.base_layer.bias', 'transformer.h.3.mlp.c_fc.base_layer.weight', 'transformer.h.3.mlp.c_fc.lora_A.default.weight', 'transformer.h.3.mlp.c_fc.lora_B.default.weight', 'transformer.h.3.mlp.c_proj.base_layer.bias', 'transformer.h.3.mlp.c_proj.base_layer.weight', 'transformer.h.3.mlp.c_proj.lora_A.default.weight', 'transformer.h.3.mlp.c_proj.lora_B.default.weight', 'transformer.h.4.attn.c_attn.base_layer.bias', 'transformer.h.4.attn.c_attn.base_layer.weight', 'transformer.h.4.attn.c_attn.lora_A.default.weight', 'transformer.h.4.attn.c_attn.lora_B.default.weight', 'transformer.h.4.attn.c_proj.base_layer.bias', 'transformer.h.4.attn.c_proj.base_layer.weight', 'transformer.h.4.attn.c_proj.lora_A.default.weight', 'transformer.h.4.attn.c_proj.lora_B.default.weight', 'transformer.h.4.mlp.c_fc.base_layer.bias', 'transformer.h.4.mlp.c_fc.base_layer.weight', 'transformer.h.4.mlp.c_fc.lora_A.default.weight', 'transformer.h.4.mlp.c_fc.lora_B.default.weight', 'transformer.h.4.mlp.c_proj.base_layer.bias', 'transformer.h.4.mlp.c_proj.base_layer.weight', 'transformer.h.4.mlp.c_proj.lora_A.default.weight', 'transformer.h.4.mlp.c_proj.lora_B.default.weight', 'transformer.h.5.attn.c_attn.base_layer.bias', 'transformer.h.5.attn.c_attn.base_layer.weight', 'transformer.h.5.attn.c_attn.lora_A.default.weight', 'transformer.h.5.attn.c_attn.lora_B.default.weight', 'transformer.h.5.attn.c_proj.base_layer.bias', 'transformer.h.5.attn.c_proj.base_layer.weight', 'transformer.h.5.attn.c_proj.lora_A.default.weight', 'transformer.h.5.attn.c_proj.lora_B.default.weight', 'transformer.h.5.mlp.c_fc.base_layer.bias', 'transformer.h.5.mlp.c_fc.base_layer.weight', 'transformer.h.5.mlp.c_fc.lora_A.default.weight', 'transformer.h.5.mlp.c_fc.lora_B.default.weight', 'transformer.h.5.mlp.c_proj.base_layer.bias', 'transformer.h.5.mlp.c_proj.base_layer.weight', 'transformer.h.5.mlp.c_proj.lora_A.default.weight', 'transformer.h.5.mlp.c_proj.lora_B.default.weight', 'transformer.h.6.attn.c_attn.base_layer.bias', 'transformer.h.6.attn.c_attn.base_layer.weight', 'transformer.h.6.attn.c_attn.lora_A.default.weight', 'transformer.h.6.attn.c_attn.lora_B.default.weight', 'transformer.h.6.attn.c_proj.base_layer.bias', 'transformer.h.6.attn.c_proj.base_layer.weight', 'transformer.h.6.attn.c_proj.lora_A.default.weight', 'transformer.h.6.attn.c_proj.lora_B.default.weight', 'transformer.h.6.mlp.c_fc.base_layer.bias', 'transformer.h.6.mlp.c_fc.base_layer.weight', 'transformer.h.6.mlp.c_fc.lora_A.default.weight', 'transformer.h.6.mlp.c_fc.lora_B.default.weight', 'transformer.h.6.mlp.c_proj.base_layer.bias', 'transformer.h.6.mlp.c_proj.base_layer.weight', 'transformer.h.6.mlp.c_proj.lora_A.default.weight', 'transformer.h.6.mlp.c_proj.lora_B.default.weight', 'transformer.h.7.attn.c_attn.base_layer.bias', 'transformer.h.7.attn.c_attn.base_layer.weight', 'transformer.h.7.attn.c_attn.lora_A.default.weight', 'transformer.h.7.attn.c_attn.lora_B.default.weight', 'transformer.h.7.attn.c_proj.base_layer.bias', 'transformer.h.7.attn.c_proj.base_layer.weight', 'transformer.h.7.attn.c_proj.lora_A.default.weight', 'transformer.h.7.attn.c_proj.lora_B.default.weight', 'transformer.h.7.mlp.c_fc.base_layer.bias', 'transformer.h.7.mlp.c_fc.base_layer.weight', 'transformer.h.7.mlp.c_fc.lora_A.default.weight', 'transformer.h.7.mlp.c_fc.lora_B.default.weight', 'transformer.h.7.mlp.c_proj.base_layer.bias', 'transformer.h.7.mlp.c_proj.base_layer.weight', 'transformer.h.7.mlp.c_proj.lora_A.default.weight', 'transformer.h.7.mlp.c_proj.lora_B.default.weight', 'transformer.h.8.attn.c_attn.base_layer.bias', 'transformer.h.8.attn.c_attn.base_layer.weight', 'transformer.h.8.attn.c_attn.lora_A.default.weight', 'transformer.h.8.attn.c_attn.lora_B.default.weight', 'transformer.h.8.attn.c_proj.base_layer.bias', 'transformer.h.8.attn.c_proj.base_layer.weight', 'transformer.h.8.attn.c_proj.lora_A.default.weight', 'transformer.h.8.attn.c_proj.lora_B.default.weight', 'transformer.h.8.mlp.c_fc.base_layer.bias', 'transformer.h.8.mlp.c_fc.base_layer.weight', 'transformer.h.8.mlp.c_fc.lora_A.default.weight', 'transformer.h.8.mlp.c_fc.lora_B.default.weight', 'transformer.h.8.mlp.c_proj.base_layer.bias', 'transformer.h.8.mlp.c_proj.base_layer.weight', 'transformer.h.8.mlp.c_proj.lora_A.default.weight', 'transformer.h.8.mlp.c_proj.lora_B.default.weight', 'transformer.h.9.attn.c_attn.base_layer.bias', 'transformer.h.9.attn.c_attn.base_layer.weight', 'transformer.h.9.attn.c_attn.lora_A.default.weight', 'transformer.h.9.attn.c_attn.lora_B.default.weight', 'transformer.h.9.attn.c_proj.base_layer.bias', 'transformer.h.9.attn.c_proj.base_layer.weight', 'transformer.h.9.attn.c_proj.lora_A.default.weight', 'transformer.h.9.attn.c_proj.lora_B.default.weight', 'transformer.h.9.mlp.c_fc.base_layer.bias', 'transformer.h.9.mlp.c_fc.base_layer.weight', 'transformer.h.9.mlp.c_fc.lora_A.default.weight', 'transformer.h.9.mlp.c_fc.lora_B.default.weight', 'transformer.h.9.mlp.c_proj.base_layer.bias', 'transformer.h.9.mlp.c_proj.base_layer.weight', 'transformer.h.9.mlp.c_proj.lora_A.default.weight', 'transformer.h.9.mlp.c_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/src/models/model and are newly initialized: ['transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.12.attn.c_attn.bias', 'transformer.h.12.attn.c_attn.weight', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.attn.c_proj.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.13.attn.c_attn.bias', 'transformer.h.13.attn.c_attn.weight', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.attn.c_proj.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.attn.c_attn.bias', 'transformer.h.14.attn.c_attn.weight', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.attn.c_proj.weight', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.attn.c_attn.bias', 'transformer.h.15.attn.c_attn.weight', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.attn.c_proj.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.attn.c_attn.bias', 'transformer.h.16.attn.c_attn.weight', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.attn.c_proj.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.attn.c_attn.bias', 'transformer.h.17.attn.c_attn.weight', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.attn.c_proj.weight', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.18.attn.c_attn.bias', 'transformer.h.18.attn.c_attn.weight', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.attn.c_proj.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.19.attn.c_attn.bias', 'transformer.h.19.attn.c_attn.weight', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.attn.c_proj.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.20.attn.c_attn.bias', 'transformer.h.20.attn.c_attn.weight', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.attn.c_proj.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.21.attn.c_attn.bias', 'transformer.h.21.attn.c_attn.weight', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.attn.c_proj.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.22.attn.c_attn.bias', 'transformer.h.22.attn.c_attn.weight', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.attn.c_proj.weight', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.23.attn.c_attn.bias', 'transformer.h.23.attn.c_attn.weight', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.attn.c_proj.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_model_predictions(model, tokenizer, data):\n",
    "    \"\"\"\n",
    "    Generates predictions using the model for each sentence in the provided data.\n",
    "    :param model: GPT2 FT or PEFT model to evaluate\n",
    "    :param tokenizer: Tokenizer for the model\n",
    "    :param data: List of tokenised sentences for evaluation\n",
    "    :return: List of generated sentences\n",
    "    \"\"\"\n",
    "    generated_sentences = []\n",
    "\n",
    "    # Step 1: Generate predictions using the model for each sentence\n",
    "    with torch.no_grad():\n",
    "        for tokens in tqdm(data, desc=\"Generating sentences\", unit=\"sentence\"):\n",
    "            input_sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "            inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "            # Generate predictions using model\n",
    "            output = model.generate(input_ids, max_length=len(input_ids[0]) + 20, num_return_sequences=1)\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            generated_sentences.append(generated_text)\n",
    "\n",
    "    return generated_sentences"
   ],
   "id": "9ba19188e517479d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_predictions = get_model_predictions(lora_model, lora_tokenizer, data)",
   "id": "d71095ae09478d5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:27:55.154581Z",
     "start_time": "2024-10-30T17:27:55.145500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_perplexity_kneser_ney(generated_sentences, n=3):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of model predictions on the given data using Kneser-Ney smoothing.\n",
    "    :param generated_sentences: List of generated sentences\n",
    "    :param n: Order of the n-gram model (3 for trigram)\n",
    "    :return average_perplexity: Average perplexity score for the data\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare n-grams for Kneser-Ney\n",
    "    # Pad trigrams and prepare vocabulary for Kneser-Ney smoothing\n",
    "    train_data, vocab = padded_everygram_pipeline(n, generated_sentences)\n",
    "\n",
    "    # Initialize Kneser-Ney language model\n",
    "    kn_model = KneserNeyInterpolated(n)\n",
    "    kn_model.fit(train_data, vocab)\n",
    "\n",
    "    # Step 2: Calculate perplexity of the generated sentences\n",
    "    total_perplexity = 0\n",
    "    for sentence_tokens in tqdm(generated_sentences, desc=\"Calculating perplexity\", unit=\"sentence\"):\n",
    "        sentence_ngrams = list(ngrams(sentence_tokens, n))\n",
    "        perplexity = kn_model.perplexity(sentence_ngrams)\n",
    "        total_perplexity += perplexity\n",
    "\n",
    "    # Calculate average perplexity across all generated sentences\n",
    "    average_perplexity = total_perplexity / len(generated_sentences) if len(generated_sentences) > 0 else float('inf')\n",
    "    return average_perplexity"
   ],
   "id": "78007c9633cb1ffb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:28:06.568307Z",
     "start_time": "2024-10-30T17:27:57.462566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PPL_score = get_perplexity_kneser_ney(model_predictions)\n",
    "print(f'The perplexity of the model is: {PPL_score}')"
   ],
   "id": "578424364c8dd1a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sentences:   0%|          | 0/2101 [00:00<?, ?sentence/s]/Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/src/data_processing/Formality_Transfer_Dataset.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
      "/Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/src/data_processing/Formality_Transfer_Dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating sentences:   0%|          | 1/2101 [00:04<2:50:57,  4.88s/sentence]/Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/src/data_processing/Formality_Transfer_Dataset.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
      "/Users/lauramariaquirosconesa/Documents/BSc/Year4/Large Language Models/LLM-Group-04/src/data_processing/Formality_Transfer_Dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating sentences:   0%|          | 2/2101 [00:07<1:56:35,  3.33s/sentence]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating sentences:   0%|          | 2/2101 [00:07<2:13:05,  3.80s/sentence]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m PPL_score \u001B[38;5;241m=\u001B[39m \u001B[43mget_perplexity_kneser_ney\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlora_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlora_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe perplexity of the model is: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPPL_score\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 21\u001B[0m, in \u001B[0;36mget_perplexity_kneser_ney\u001B[0;34m(model, tokenizer, data, n)\u001B[0m\n\u001B[1;32m     18\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Generate predictions using model\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(output[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m generated_sentences\u001B[38;5;241m.\u001B[39mappend(generated_text)\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/peft/peft_model.py:1732\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.generate\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1731\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1732\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1734\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2047\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2039\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2040\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2041\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2042\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2043\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2044\u001B[0m     )\n\u001B[1;32m   2046\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2047\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2048\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2049\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2050\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2052\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2054\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2055\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2057\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2058\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2059\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2060\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2061\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2066\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2067\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3007\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3004\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[1;32m   3006\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 3007\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3009\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   3010\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/accelerate/hooks.py:170\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 170\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1316\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1309\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1310\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[1;32m   1311\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[1;32m   1312\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[1;32m   1313\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1314\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1316\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1319\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1320\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1321\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1326\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1327\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1328\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1330\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1331\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1333\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1130\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1118\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m   1119\u001B[0m         block\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m   1120\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1127\u001B[0m         output_attentions,\n\u001B[1;32m   1128\u001B[0m     )\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1130\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1133\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1139\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1141\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:651\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    648\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m outputs \u001B[38;5;241m+\u001B[39m cross_attn_outputs[\u001B[38;5;241m2\u001B[39m:]  \u001B[38;5;66;03m# add cross attentions if we output attention weights\u001B[39;00m\n\u001B[1;32m    650\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m--> 651\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln_2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    652\u001B[0m feed_forward_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(hidden_states)\n\u001B[1;32m    653\u001B[0m \u001B[38;5;66;03m# residual connection\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:201\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/BSc/Year4/Large Language Models/LLM-Group-04/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2546\u001B[0m, in \u001B[0;36mlayer_norm\u001B[0;34m(input, normalized_shape, weight, bias, eps)\u001B[0m\n\u001B[1;32m   2542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[1;32m   2543\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2544\u001B[0m         layer_norm, (\u001B[38;5;28minput\u001B[39m, weight, bias), \u001B[38;5;28minput\u001B[39m, normalized_shape, weight\u001B[38;5;241m=\u001B[39mweight, bias\u001B[38;5;241m=\u001B[39mbias, eps\u001B[38;5;241m=\u001B[39meps\n\u001B[1;32m   2545\u001B[0m     )\n\u001B[0;32m-> 2546\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_word_overlap(generated_sentences, data):\n",
    "    \"\"\"\n",
    "    This function calculates the word overlap between the original and generated sentences.\n",
    "    :param generated_sentences: the generated sentences from the GPT2 model\n",
    "    :param data: the test data from which the generated sentences were generated\n",
    "    :return overlap: the word overlap between the original and generated sentences\n",
    "    \"\"\"\n",
    "    overlap_counts = []\n",
    "\n",
    "    for original, generated in zip(data, generated_sentences):\n",
    "        # Convert the original and generated sentences to sets of words\n",
    "        original_words = set(original)  # Assuming 'original' is a list of tokens\n",
    "        generated_words = set(generated.split())  # Split generated sentence into words\n",
    "\n",
    "        # Calculate the overlap between the two sets\n",
    "        overlap = original_words.intersection(generated_words)\n",
    "\n",
    "        # Store the count of overlapping words\n",
    "        overlap_counts.append(len(overlap))\n",
    "\n",
    "    # Calculate average overlap across all sentence pairs\n",
    "    average_overlap = sum(overlap_counts) / len(overlap_counts) if overlap_counts else 0\n",
    "    return average_overlap"
   ],
   "id": "85551b52516ac351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_BERT_score(generated_sentences, data):\n",
    "    \"\"\"\n",
    "        This function calculates the BERT score between the original and generated sentences.\n",
    "        :param generated_sentences: The generated sentences from the GPT2 model\n",
    "        :param data: The original sentences (in their natural form, not tokenized) for evaluation\n",
    "        :return: BERT_score: The BERT score between the original and generated sentences\n",
    "    \"\"\"\n",
    "    # Compute BERT scores\n",
    "    P, R, F1 = BERT_score(generated_sentences, data, lang='en', verbose=True)\n",
    "\n",
    "    # Return the F1 score as a representative BERT score\n",
    "    return F1.mean().item()  # Convert tensor to a standard float"
   ],
   "id": "379fb0a06013309c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Give the content-preservation score of the model (BERT score and word overlap)\n",
    "BERT_score = get_BERT_score(model_predictions, data)\n",
    "print(f'The BERT score of the model is: {BERT_score}')\n",
    "word_overlap_average = get_word_overlap(model_predictions, data)\n",
    "print(f'The average word overlap of the model is: {word_overlap_average}')"
   ],
   "id": "a56a4c3b97765dbc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
